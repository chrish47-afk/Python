{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447450ea-a228-4e31-aa2c-11803c014004",
   "metadata": {},
   "source": [
    "# Understanding A/B Testing\n",
    "A/B testing (also called split testing) is an experiment where you compare two versions of something (e.g., a website, an email, an ad) to determine which one performs better. The goal is to make data-driven decisions.\n",
    "\n",
    "##### References\n",
    "1. https://www.abtasty.com/resources/ab-testing/#:~:text=A%2FB%20testing%2C%20also%20known,the%20rest%20to%20the%20second.\n",
    "2. https://towardsdatascience.com/a-b-testing-a-complete-guide-to-statistical-testing-e3f1db140499/\n",
    "3. https://www.invespcro.com/blog/ab-testing-statistics-made-simple/\n",
    "4. https://www.datacamp.com/blog/data-demystified-what-is-a-b-testing\n",
    "5. https://github.com/FrancescoCasalegno/AB-Testing/blob/main/AB_Testing.ipynb [A/B Testing in Python]\n",
    "\n",
    "## 1. Key Components of A/B Testing\n",
    "### A. Control Group vs. Treatment Group\n",
    "- **Control Group (A):** This group gets the current version.\n",
    "- **Treatment Group (B):** This group gets the new version.\n",
    "- Users are randomly assigned to either group and performance metrics are collected.\n",
    "\n",
    "### B. Key Metrics to Measure\n",
    "- **Conversion Rate (CR):** Percentage of users who complete a desired action.\n",
    "- **Click-Through Rate (CTR):** Percentage of users who click a link or button.\n",
    "- **Revenue per Visitor (RPV):** Revenue generated per visitor.\n",
    "\n",
    "**Formula for Conversion Rate (CR):**\n",
    "```r\n",
    "CR <- function(conversions, visitors) {\n",
    "  return (conversions / visitors)\n",
    "}\n",
    "CR(120, 1000)  # Example: 120/1000 = 12%\n",
    "```\n",
    "\n",
    "## 2. Important Considerations for a Good A/B Test\n",
    "### A. Randomization\n",
    "- Users must be randomly assigned to either group to prevent bias.\n",
    "\n",
    "### B. Sample Size & Statistical Power\n",
    "- Ensure a large enough sample size to detect meaningful differences.\n",
    "- Common significance level (Î±) and power:\n",
    "  - Î± = 0.05 (5% risk of false positives)\n",
    "  - Power = 0.8 (80% chance of detecting real differences)\n",
    "\n",
    "### C. Statistical Significance (p-value)\n",
    "- **Null Hypothesis (Hâ‚€):** \"No difference between A and B.\"\n",
    "- **Alternative Hypothesis (Hâ‚):** \"There is a difference.\"\n",
    "- A p-value < 0.05 means we reject Hâ‚€.\n",
    "\n",
    "### D. Type I & Type II Errors\n",
    "- **Type I Error (False Positive):** Wrongly detecting a difference.\n",
    "- **Type II Error (False Negative):** Failing to detect a real difference.\n",
    "\n",
    "## 3. Statistical Methods Used in A/B Testing\n",
    "### A. Two-Sample t-Test\n",
    "**Definition:** The t-test is used to compare the means of two groups to determine if there is a statistically significant difference between them. \n",
    "- The two-sample t-test (also called an independent t-test) is used to compare the means of two groups. In A/B testing, it helps determine if a numerical metric (like average time spent on a page, revenue per user, etc.) is significantly different between two variations.\n",
    "\n",
    "**Assumptions**\n",
    "- Independence â€“ The samples must be independent.\n",
    "- Normality â€“ The data should be normally distributed (if sample size is large, the Central Limit Theorem applies).\n",
    "- Equal Variances (for standard t-test) â€“ If variances are unequal, we use Welchâ€™s t-test.\n",
    "    \n",
    "**Pros:**\n",
    "- Simple and widely used.\n",
    "- Works well with normally distributed data.\n",
    "\n",
    "**Cons:**\n",
    "- Assumes normality and equal variance in groups.\n",
    "- Less effective for small sample sizes.\n",
    "\n",
    "**Best Scenario to Use:**\n",
    "- When comparing continuous numerical data, such as average revenue per user or time spent on a webpage.\n",
    "\n",
    "```r\n",
    "# Simulated A/B test data\n",
    "a <- rbinom(1000, 1, 0.13)  # 13% conversion rate\n",
    "b <- rbinom(1000, 1, 0.15)  # 15% conversion rate\n",
    "\n",
    "# Perform t-test\n",
    "t_test <- t.test(a, b)\n",
    "print(t_test)\n",
    "```\n",
    "\n",
    "**When to Use a t-Test?**\n",
    "- Comparing average spending per user.\n",
    "- Comparing time spent on a webpage between two versions.\n",
    "- Comparing average order value.\n",
    "    \n",
    "### B. Chi-Square Test\n",
    "**Definition:** The chi-square test is used to determine if there is a significant association between categorical variables, often used for comparing proportions.\n",
    "- The Chi-Square test is used to compare proportions between two groups. In A/B testing, it is commonly used to compare conversion rates (e.g., how many users clicked a button or made a purchase).\n",
    "\n",
    "**Assumptions**\n",
    "- Data is categorical (e.g., converted vs. not converted).\n",
    "- Expected frequency in each cell is at least 5 for reliable results.                                                                                                                                          \n",
    "                                                                                                                                          \n",
    "**Pros:**\n",
    "- Works well for categorical data (e.g., conversion rates, user actions).\n",
    "- Does not assume a normal distribution.\n",
    "\n",
    "**Cons:**\n",
    "- Requires sufficiently large sample sizes.\n",
    "- Not ideal for very small data samples due to unstable results.\n",
    "\n",
    "**Best Scenario to Use:**\n",
    "- When comparing conversion rates between groups (e.g., click-through rates for A vs. B).\n",
    "\n",
    "```r\n",
    "# Simulated conversion data\n",
    "observed <- matrix(c(200, 800, 250, 750), nrow=2, byrow=TRUE)\n",
    "colnames(observed) <- c(\"Converted\", \"Not Converted\")\n",
    "rownames(observed) <- c(\"A\", \"B\")\n",
    "\n",
    "# Perform chi-square test\n",
    "chi_test <- chisq.test(observed)\n",
    "print(chi_test)\n",
    "```\n",
    "**When to Use a Chi-Square Test?**\n",
    "- Comparing click-through rates.\n",
    "- Comparing signup rates between two versions.\n",
    "- Checking if conversion rates differ between A and B.\n",
    "\n",
    "### C. Bayesian A/B Testing\n",
    "**Definition:** Bayesian A/B testing uses probability distributions (Beta distributions) to model conversion rates and determine the probability of one variation being better than another.\n",
    "- Instead of a binary \"reject/fail to reject\" decision like t-tests, Bayesian A/B testing provides the probability that one variant is better than the other.\n",
    "\n",
    "**How Does it Work?**\n",
    "- We model the conversion rate as a Beta distribution.\n",
    "- We update beliefs with observed data using Bayes' Theorem.\n",
    "- We compute the probability that B is better than A.\n",
    "    \n",
    "**Pros:**\n",
    "- Provides a probability of A being better than B rather than a binary yes/no decision.\n",
    "- More intuitive for decision-making.\n",
    "\n",
    "**Cons:**\n",
    "- More computationally intensive.\n",
    "- Requires careful choice of priors.\n",
    "\n",
    "**Best Scenario to Use:**\n",
    "- When decision-makers prefer a probability-based approach instead of relying on p-values.\n",
    "- When data is limited, and prior knowledge can help improve estimates.\n",
    "\n",
    "```r\n",
    "# Install required package\n",
    "install.packages(\"bayesAB\")\n",
    "library(bayesAB)\n",
    "\n",
    "# Simulated Bayesian A/B Test\n",
    "a_success <- 200\n",
    "a_trials <- 1000\n",
    "b_success <- 250\n",
    "b_trials <- 1000\n",
    "\n",
    "bayes_result <- bayesTest(\n",
    "  a_success, a_trials,\n",
    "  b_success, b_trials,\n",
    "  priors = c(\"alpha\" = 1, \"beta\" = 1),\n",
    "  n_samples = 50000\n",
    ")\n",
    "\n",
    "print(summary(bayes_result))\n",
    "plot(bayes_result)\n",
    "```\n",
    "**When to Use Bayesian A/B Testing?**\n",
    "- When you need a probability estimate instead of a binary decision.\n",
    "- When sample sizes are small.\n",
    "- When decision-makers prefer a risk-based approach.\n",
    "                      \n",
    "## 4. Conclusion\n",
    "- **t-tests** and **chi-square tests** are commonly used.\n",
    "- **Bayesian methods** provide probability-based insights.\n",
    "- **Randomization** ensures fairness.\n",
    "- **Adequate sample size** prevents misleading results.\n",
    "- **Stopping tests too early** can lead to incorrect conclusions.\n",
    "\n",
    "This R Markdown file serves as a complete documentation for A/B testing principles, methods, and practical implementations. ðŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2292f862-d15d-4ab7-a611-53ce45a09558",
   "metadata": {},
   "source": [
    "# A/B Testing Cheat Sheet\n",
    "\n",
    "## 1. Selecting Metrics for Experimentation\n",
    "Choosing the right metrics ensures meaningful results. Consider:\n",
    "- **Primary Metric**: The main success indicator (e.g., conversion rate, revenue per user).\n",
    "- **Secondary Metrics**: Additional insights (e.g., bounce rate, average session duration).\n",
    "- **Leading Indicators**: Early signals that correlate with long-term impact.\n",
    "- **Guardrail Metrics**: Ensures the experiment does not negatively impact other areas (e.g., page load time, retention rate).\n",
    "\n",
    "**Best Practices:**\n",
    "- Align metrics with business goals.\n",
    "- Use metrics that are directly impacted by the test.\n",
    "- Avoid vanity metrics (e.g., total page views if engagement matters more).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Selecting Randomization Units\n",
    "Randomization ensures fairness in group assignment. Common units include:\n",
    "- **User-Level Randomization** (e.g., individual visitors, accounts): Ideal for personalized experiences.\n",
    "- **Session-Level Randomization**: Good for short-lived experiments but may introduce bias from repeat users.\n",
    "- **Device-Level Randomization**: Used when the user experience differs across devices.\n",
    "- **Geographic/Regional Randomization**: When running country-specific experiments.\n",
    "- **Cookie-Based Randomization**: Ensures consistent treatment per browser session.\n",
    "\n",
    "**Best Practices:**\n",
    "- Ensure each unit has an equal chance of being assigned.\n",
    "- Minimize cross-group contamination.\n",
    "- Consider user behavior (e.g., multi-device users may need user-level randomization).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Choosing a Target Population\n",
    "Defining your target audience ensures valid, actionable results.\n",
    "- **General Population:** Testing on all users to understand broad impact.\n",
    "- **New Users:** Measuring the effect on first-time visitors.\n",
    "- **Returning Users:** Understanding impact on loyal customers.\n",
    "- **High-Intent Users:** Users already close to conversion.\n",
    "- **Geographic Segments:** Comparing behavior across different regions.\n",
    "\n",
    "**Best Practices:**\n",
    "- Ensure the sample is representative of your actual audience.\n",
    "- Exclude anomalies (e.g., internal users, bot traffic).\n",
    "- Consider external factors (e.g., seasonality, market trends).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Computing Sample Size\n",
    "A proper sample size ensures statistical reliability.\n",
    "\n",
    "**Key Inputs:**\n",
    "- **Baseline Conversion Rate (pâ‚€)**: The current conversion rate.\n",
    "- **Minimum Detectable Effect (MDE)**: The smallest effect worth detecting.\n",
    "- **Statistical Power (1-Î²)**: Usually set at 80%.\n",
    "- **Significance Level (Î±)**: Typically set at 5% (p < 0.05).\n",
    "\n",
    "**Formula for Sample Size Per Group:**\n",
    "```r\n",
    "n <- function(p0, MDE, alpha=0.05, power=0.8) {\n",
    "  z_alpha <- qnorm(1 - alpha / 2)\n",
    "  z_beta <- qnorm(power)\n",
    "  return ((2 * p0 * (1 - p0) * (z_alpha + z_beta)^2) / MDE^2)\n",
    "}\n",
    "n(0.1, 0.02) # Example calculation\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "- Use online calculators (e.g., Optimizely, Evan Miller's calculator).\n",
    "- Ensure the test has enough power to detect real changes.\n",
    "- If results are inconclusive, increase sample size rather than reducing MDE.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Determining Test Duration\n",
    "The test duration depends on sample size, traffic, and expected conversion rates.\n",
    "\n",
    "**Steps to Determine Duration:**\n",
    "1. **Estimate daily visitors** (per variant).\n",
    "2. **Compute required sample size** (from previous step).\n",
    "3. **Divide sample size by daily traffic** to get the test duration.\n",
    "\n",
    "**Best Practices:**\n",
    "- Run tests for at least **1-2 full business cycles** (to capture weekly patterns).\n",
    "- Avoid stopping tests too early based on interim results.\n",
    "- Use a sequential analysis approach for early stopping when necessary.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Analyzing Results\n",
    "Once data is collected, statistical analysis determines if the observed differences are significant.\n",
    "\n",
    "**Common Tests:**\n",
    "- **Two-Sample t-Test**: Compares means (e.g., revenue per user).\n",
    "- **Chi-Square Test**: Compares categorical data (e.g., conversion rates).\n",
    "- **Bayesian Analysis**: Provides probability estimates rather than binary outcomes.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **P-Value < 0.05**: Suggests statistical significance.\n",
    "- **Confidence Interval (CI)**: Helps understand the range of true effects.\n",
    "- **Lift Calculation**: Measures improvement between A and B.\n",
    "```r\n",
    "lift <- function(rate_A, rate_B) {\n",
    "  return ((rate_B - rate_A) / rate_A * 100)\n",
    "}\n",
    "lift(0.1, 0.12) # Example calculation\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "- Ensure results remain stable before making decisions.\n",
    "- Segment results (e.g., by device, region) to uncover hidden trends.\n",
    "- Validate with a post-test analysis (e.g., long-term impact, retention).\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Alternatives to A/B Testing\n",
    "If A/B testing is not feasible, consider alternative methods:\n",
    "- **Multi-Armed Bandit (MAB)**: Adapts allocation dynamically to maximize performance.\n",
    "- **Sequential Testing**: Allows early stopping if results are conclusive.\n",
    "- **Synthetic Control Method**: Used when randomization is impractical.\n",
    "- **Difference-in-Differences (DID)**: Compares trends over time between test and control groups.\n",
    "- **Pre-Post Analysis**: Compares metrics before and after changes (used when A/B testing isn't possible).\n",
    "\n",
    "**Best Practices:**\n",
    "- Use MAB when time-sensitive optimizations are needed.\n",
    "- Use DID for observational data where A/B testing is infeasible.\n",
    "- Pre-Post works for **major redesigns** but is less reliable than randomized experiments.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Thoughts\n",
    "A/B testing is a powerful tool when done correctly. Following structured methodologies ensures reliable, data-driven decisions. Always validate results, and consider alternatives when necessary.\n",
    "\n",
    "---\n",
    "# A/B Testing Interview Cheat Sheet\n",
    "\n",
    "## 1. Fundamentals of A/B Testing\n",
    "**Q: What is A/B testing?**\n",
    "- A/B testing (split testing) is a controlled experiment comparing two versions (A and B) to determine which performs better based on predefined metrics.\n",
    "\n",
    "**Q: Why is A/B testing important?**\n",
    "- Helps in data-driven decision-making.\n",
    "- Reduces guesswork in product and marketing changes.\n",
    "- Measures the impact of changes before rolling them out widely.\n",
    "\n",
    "**Q: What are some common use cases?**\n",
    "- Optimizing website design (CTA buttons, landing pages).\n",
    "- Testing different email subject lines for engagement.\n",
    "- Comparing product pricing strategies.\n",
    "- Improving user experience in apps.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Designing an A/B Test\n",
    "**Q: How do you select metrics for an A/B test?**\n",
    "- **Primary Metric**: The main success indicator (e.g., conversion rate, revenue per user).\n",
    "- **Secondary Metrics**: Additional insights (e.g., session duration, bounce rate).\n",
    "- **Guardrail Metrics**: Ensures no unintended negative impact (e.g., page load speed).\n",
    "\n",
    "**Q: How do you select randomization units?**\n",
    "- **User-level**: Most common, ensures users experience only one variant.\n",
    "- **Session-level**: Used when the test is short-lived.\n",
    "- **Device-level**: When experiments need to be device-consistent.\n",
    "- **Geographic-level**: Used for regional campaigns.\n",
    "\n",
    "**Q: What factors influence the target population?**\n",
    "- New vs. returning users.\n",
    "- High-intent vs. casual visitors.\n",
    "- Geographic or demographic segments.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Statistical Concepts in A/B Testing\n",
    "**Q: What is statistical significance?**\n",
    "- Measures whether observed differences are due to chance.\n",
    "- A **p-value < 0.05** typically means results are statistically significant.\n",
    "\n",
    "**Q: What is the difference between Type I and Type II errors?**\n",
    "- **Type I Error (False Positive):** Incorrectly rejecting the null hypothesis.\n",
    "- **Type II Error (False Negative):** Failing to detect a real difference.\n",
    "\n",
    "**Q: How do you calculate sample size for an A/B test?**\n",
    "- Based on **baseline conversion rate**, **minimum detectable effect (MDE)**, **statistical power (80%)**, and **significance level (5%)**.\n",
    "- Formula:\n",
    "  ```r\n",
    "  n <- function(p0, MDE, alpha=0.05, power=0.8) {\n",
    "    z_alpha <- qnorm(1 - alpha / 2)\n",
    "    z_beta <- qnorm(power)\n",
    "    return ((2 * p0 * (1 - p0) * (z_alpha + z_beta)^2) / MDE^2)\n",
    "  }\n",
    "  n(0.1, 0.02) # Example calculation\n",
    "  ```\n",
    "\n",
    "**Q: What is the lift in A/B testing?**\n",
    "- **Lift** measures the percentage increase in the metric between A and B.\n",
    "  ```r\n",
    "  lift <- function(rate_A, rate_B) {\n",
    "    return ((rate_B - rate_A) / rate_A * 100)\n",
    "  }\n",
    "  lift(0.1, 0.12) # Example calculation\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Running and Analyzing an A/B Test\n",
    "**Q: How long should an A/B test run?**\n",
    "- Run until **statistical significance is reached** or **a full business cycle** is completed.\n",
    "- Consider seasonality and traffic fluctuations.\n",
    "\n",
    "**Q: How do you analyze A/B test results?**\n",
    "- **Two-Sample t-Test**: Compares means (e.g., revenue per user).\n",
    "- **Chi-Square Test**: Compares categorical data (e.g., conversion rates).\n",
    "- **Bayesian Analysis**: Provides probability estimates instead of p-values.\n",
    "\n",
    "**Q: What do you do if your A/B test results are inconclusive?**\n",
    "- Increase sample size.\n",
    "- Test a larger effect size.\n",
    "- Analyze different segments.\n",
    "- Consider external factors (e.g., seasonality, traffic sources).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Challenges and Pitfalls\n",
    "**Q: What are some common mistakes in A/B testing?**\n",
    "- **Stopping the test too early**: Leads to misleading conclusions.\n",
    "- **Multiple testing problem**: Running many tests increases false positives.\n",
    "- **Ignoring interaction effects**: Some tests impact others (e.g., pricing vs. UI changes).\n",
    "- **Poor randomization**: Biased test groups can invalidate results.\n",
    "\n",
    "**Q: How do you handle low sample sizes?**\n",
    "- Use **Bayesian methods** instead of frequentist approaches.\n",
    "- Run tests on **high-traffic pages** for faster results.\n",
    "- Consider **Multi-Armed Bandits** to dynamically allocate traffic.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Alternatives to A/B Testing\n",
    "**Q: What can you do if an A/B test isnâ€™t feasible?**\n",
    "- **Multi-Armed Bandit (MAB):** Dynamically allocates traffic to better-performing variants.\n",
    "- **Sequential Testing:** Allows stopping tests early based on statistical thresholds.\n",
    "- **Difference-in-Differences (DID):** Compares trends over time instead of random assignment.\n",
    "- **Synthetic Control Method:** Used when true randomization is not possible.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Final Tips for Acing A/B Testing Interviews\n",
    "- Be clear on **business objectives** before discussing test setup.\n",
    "- Explain **why a test is needed** and **how success is measured**.\n",
    "- Show an understanding of **both statistical and practical significance**.\n",
    "- Be ready to discuss **edge cases** (e.g., how to handle bot traffic, seasonality, or experiment conflicts).\n",
    "- Mention **real-world challenges** and how youâ€™d address them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "783c0f55-b5eb-4e35-9975-2ad518ffecab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Rate A: 13.40%\n",
      "Conversion Rate B: 15.20%\n",
      "T-statistic: -1.1495\n",
      "P-value: 0.2505\n",
      "Fail to reject the null hypothesis: No significant difference between the two versions.\n"
     ]
    }
   ],
   "source": [
    "##### Simple Example using the two-smaple t test\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Simulate conversion data\n",
    "np.random.seed(42)\n",
    "conversions_A = np.random.binomial(1, 0.13, 1000)  # 13% conversion rate\n",
    "conversions_B = np.random.binomial(1, 0.15, 1000)  # 15% conversion rate\n",
    "\n",
    "# Calculate conversion rates\n",
    "conversion_rate_A = conversions_A.mean()\n",
    "conversion_rate_B = conversions_B.mean()\n",
    "print(f'Conversion Rate A: {conversion_rate_A:.2%}')\n",
    "print(f'Conversion Rate B: {conversion_rate_B:.2%}')\n",
    "\n",
    "# Perform a two-sample t-test\n",
    "t_stat, p_value = stats.ttest_ind(conversions_A, conversions_B)\n",
    "print(f'T-statistic: {t_stat:.4f}')\n",
    "print(f'P-value: {p_value:.4f}')\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print('Reject the null hypothesis: There is a significant difference between the two versions.')\n",
    "else:\n",
    "    print('Fail to reject the null hypothesis: No significant difference between the two versions.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d800cdfa-5777-4135-8fb4-feb50217b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Load real-world external dataset (example: marketing campaign data)\n",
    "url = \"https://raw.githubusercontent.com/anshooarora/AB_Testing_Dataset/master/ab_data.csv\"\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(StringIO(response.text))\n",
    "\n",
    "# Preprocess the data\n",
    "data = data[data['group'].isin(['control', 'treatment'])]\n",
    "data['converted'] = data['converted'].astype(int)\n",
    "\n",
    "# Split into groups\n",
    "control = data[data['group'] == 'control']['converted']\n",
    "treatment = data[data['group'] == 'treatment']['converted']\n",
    "\n",
    "# Perform Two-Sample t-Test\n",
    "def perform_t_test(control, treatment):\n",
    "    t_stat, p_value = stats.ttest_ind(control, treatment)\n",
    "    print(\"\\nTwo-Sample t-Test Results:\")\n",
    "    print(f\"T-Statistic: {t_stat:.4f}, P-Value: {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"Statistically significant difference detected!\")\n",
    "    else:\n",
    "        print(\"No statistically significant difference detected.\")\n",
    "\n",
    "perform_t_test(control, treatment)\n",
    "\n",
    "# Perform Chi-Square Test\n",
    "def perform_chi_square_test(control, treatment):\n",
    "    obs = np.array([[sum(control), len(control) - sum(control)],\n",
    "                    [sum(treatment), len(treatment) - sum(treatment)]])\n",
    "    chi2, p, dof, expected = stats.chi2_contingency(obs)\n",
    "    print(\"\\nChi-Square Test Results:\")\n",
    "    print(f\"Chi-Square Statistic: {chi2:.4f}, P-Value: {p:.4f}\")\n",
    "    if p < 0.05:\n",
    "        print(\"Statistically significant difference detected!\")\n",
    "    else:\n",
    "        print(\"No statistically significant difference detected.\")\n",
    "\n",
    "perform_chi_square_test(control, treatment)\n",
    "\n",
    "# Perform Bayesian A/B Testing\n",
    "def perform_bayesian_ab_test(success_A, trials_A, success_B, trials_B):\n",
    "    with pm.Model():\n",
    "        p_A = pm.Beta(\"p_A\", alpha=1, beta=1)\n",
    "        p_B = pm.Beta(\"p_B\", alpha=1, beta=1)\n",
    "        \n",
    "        obs_A = pm.Binomial(\"obs_A\", n=trials_A, p=p_A, observed=success_A)\n",
    "        obs_B = pm.Binomial(\"obs_B\", n=trials_B, p=p_B, observed=success_B)\n",
    "        \n",
    "        trace = pm.sample(2000, return_inferencedata=True, progressbar=False)\n",
    "    \n",
    "    pm.plot_posterior(trace, var_names=[\"p_A\", \"p_B\"])\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nRunning Bayesian A/B Testing...\")\n",
    "perform_bayesian_ab_test(sum(control), len(control), sum(treatment), len(treatment))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
